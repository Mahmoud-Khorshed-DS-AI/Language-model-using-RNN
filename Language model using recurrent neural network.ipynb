{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:27.923726Z",
     "iopub.status.busy": "2022-04-05T18:58:27.923415Z",
     "iopub.status.idle": "2022-04-05T18:58:27.936722Z",
     "shell.execute_reply": "2022-04-05T18:58:27.935783Z",
     "shell.execute_reply.started": "2022-04-05T18:58:27.923685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "# keras tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "#required layers\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,SimpleRNN, GRU, Dropout, Bidirectional\n",
    "#to convert the output to one hot encoded data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#nestrouv adam optimizer\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "#to convert tokens to ids\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:29.869462Z",
     "iopub.status.busy": "2022-04-05T18:58:29.869189Z",
     "iopub.status.idle": "2022-04-05T18:58:29.971431Z",
     "shell.execute_reply": "2022-04-05T18:58:29.970706Z",
     "shell.execute_reply.started": "2022-04-05T18:58:29.869430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#the source of the data used https://www.gutenberg.org/files/1497/1497-h/1497-h.htm#link2H_4_0004\n",
    "file = open(\".your-path-here/republic_clean.txt\",mode=\"r\", encoding = \"utf8\")\n",
    "#convert each line to item in a list\n",
    "lines = file.read().splitlines()\n",
    "#join them again into one string\n",
    "lines = ' '.join(lines)\n",
    "\n",
    "lines = lines.replace('--', ' ')\n",
    "# remove punctuations\n",
    "lines = re.sub(r'[^\\w\\s]','',lines)\n",
    "#remove more than followed speace\n",
    "lines = re.sub(' +', ' ', lines)\n",
    "lines = lines.split()\n",
    "\n",
    "#if the text has 'BOOK I' title keep the followed line, if doesn't has this title hash the followed code.\n",
    "lines = lines[2:]\n",
    "#remove non-alphabetic word in the text\n",
    "lines = [word for word in lines if word.isalpha()]\n",
    "#slice the text data\n",
    "test_lines = lines[550:651]\n",
    "print(f\"number of words in the text file after cleaning is : {len(lines)}\")\n",
    "print(f\"number of words in the test file after cleaning is : {len(test_lines)}\")\n",
    "\n",
    "#convert them to one text again\n",
    "lines = ' '.join(lines)\n",
    "test_lines = ' '.join(test_lines)\n",
    "#convert them to lower case\n",
    "lines = lines.lower()\n",
    "test_lines = test_lines.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue in preprocessing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:31.911648Z",
     "iopub.status.busy": "2022-04-05T18:58:31.910854Z",
     "iopub.status.idle": "2022-04-05T18:58:32.045255Z",
     "shell.execute_reply": "2022-04-05T18:58:32.044481Z",
     "shell.execute_reply.started": "2022-04-05T18:58:31.911607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fit on the data\n",
    "tokenizer.fit_on_texts([lines])\n",
    "#convert tokens to sequences\n",
    "IDs = np.array(tokenizer.texts_to_sequences([lines])[0])\n",
    "#change their type to int16 to consume less memory\n",
    "IDs = IDs.astype('int16')\n",
    "print(f\"number of IDs is: {len(IDs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:34.160695Z",
     "iopub.status.busy": "2022-04-05T18:58:34.158981Z",
     "iopub.status.idle": "2022-04-05T18:58:34.165966Z",
     "shell.execute_reply": "2022-04-05T18:58:34.165113Z",
     "shell.execute_reply.started": "2022-04-05T18:58:34.160643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"number of unique words is: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the data into organized sequences each sequence has 51 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:38.055977Z",
     "iopub.status.busy": "2022-04-05T18:58:38.055438Z",
     "iopub.status.idle": "2022-04-05T18:58:38.287012Z",
     "shell.execute_reply": "2022-04-05T18:58:38.286205Z",
     "shell.execute_reply.started": "2022-04-05T18:58:38.055939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in tqdm(range(50,len(IDs))):\n",
    "    sequence = IDs[i-50:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('the total number of sequences is:',len(sequences))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "print(f\"The sequences shape is {sequences.shape}\")\n",
    "print(f\"The number of words in each sequence is: {sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X will hold the 50 input words and y will hold the output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:45.636240Z",
     "iopub.status.busy": "2022-04-05T18:58:45.635717Z",
     "iopub.status.idle": "2022-04-05T18:58:45.926002Z",
     "shell.execute_reply": "2022-04-05T18:58:45.925182Z",
     "shell.execute_reply.started": "2022-04-05T18:58:45.636202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in tqdm(sequences):\n",
    "    X.append(i[0:-1])\n",
    "    y.append(i[-1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(f\"The number of sequences in the input variable is: {X.shape[0]} and the number of input words in each sequence is {X.shape[1]}\")\n",
    "print(f\"The number of sequences in the output variable is: {y.shape[0]} and each sequence has one output word\\n\")\n",
    "\n",
    "print(f\"First sequence is: {X[0]}, and it has {len(X[0])} words\")\n",
    "print(f\"and the response is: {y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:47.622651Z",
     "iopub.status.busy": "2022-04-05T18:58:47.621909Z",
     "iopub.status.idle": "2022-04-05T18:58:47.863653Z",
     "shell.execute_reply": "2022-04-05T18:58:47.862887Z",
     "shell.execute_reply.started": "2022-04-05T18:58:47.622594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Convert the output to one hot encoded data.\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:50.764967Z",
     "iopub.status.busy": "2022-04-05T18:58:50.764527Z",
     "iopub.status.idle": "2022-04-05T18:58:50.773293Z",
     "shell.execute_reply": "2022-04-05T18:58:50.772646Z",
     "shell.execute_reply.started": "2022-04-05T18:58:50.764932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_performance(history):\n",
    "    \"\"\"\n",
    "    function to plot training price loss vs validation price loss and training price accuracy vs validation price accuracy.<br>\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    history: model.fit object\n",
    "    \n",
    "    return:\n",
    "    \n",
    "    None\n",
    "    \"\"\"\n",
    "    val_loss_per_epoch = history.history['val_loss']\n",
    "    loss_per_epoch = history.history['loss']\n",
    "    val_accuracy_per_epoch = history.history['val_categorical_accuracy']\n",
    "    accuracy_per_epoch = history.history['categorical_accuracy']\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(f\"Training loss & validation loss with batch size 200\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss function')\n",
    "    plt.plot(np.arange(1,len(val_loss_per_epoch)+1),val_loss_per_epoch,label=f\"validation loss\")\n",
    "    plt.plot(np.arange(1,len(loss_per_epoch)+1),loss_per_epoch,label = f\"training loss\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(f\"Training accuracy & validation accuracy with batch size 200\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(np.arange(1,len(val_accuracy_per_epoch)+1),val_accuracy_per_epoch,label=\"validation accuracy\")\n",
    "    plt.plot(np.arange(1,len(accuracy_per_epoch)+1),accuracy_per_epoch,label = \"training accuracy\")\n",
    "    #plt.xticks(np.arange(0, 55, 5))\n",
    "    #plt.yticks(np.arange(0, 105, 5))\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:58:51.796259Z",
     "iopub.status.busy": "2022-04-05T18:58:51.795784Z",
     "iopub.status.idle": "2022-04-05T18:58:51.807073Z",
     "shell.execute_reply": "2022-04-05T18:58:51.806341Z",
     "shell.execute_reply.started": "2022-04-05T18:58:51.796225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_next_word_proba(recurrent_model, tokenizer, data):\n",
    "  #convert text to array\n",
    "  data = data.split()\n",
    "  #last word will be the true value\n",
    "  correct_word = data[-1]\n",
    "  #take last 50 words to be the test data\n",
    "  data = data[50:-1]\n",
    "\n",
    "  #convert to data to string\n",
    "  data = ' '.join(data)\n",
    "  #convert them to id sequences using our predefined tokenizer\n",
    "  sequence = tokenizer.texts_to_sequences([data])\n",
    "  #convert test sequence to numpy array\n",
    "  sequence = np.array(sequence)\n",
    "  #start predict the probability of the next word\n",
    "  proba = recurrent_model.predict(sequence)['next_word']\n",
    "  #sort the probabilities\n",
    "  sorted_proba = np.sort(recurrent_model.predict(sequence)['next_word'][0],kind = 'mergesort')\n",
    "  L = np.argsort(-proba)\n",
    "  #get the index of largest probability\n",
    "  largest_value_index = L[:,0]\n",
    "  #get the index of the second largest probability\n",
    "  second_largest_value_index = L[:,1]\n",
    "  #get the index of third largest probability\n",
    "  third_largest_value_index = L[:,2]\n",
    "  #get the largest probability\n",
    "  largest_proba = sorted_proba[-1]\n",
    "  #get the second largest probability\n",
    "  second_largest_proba = sorted_proba[-2]\n",
    "  #get the third largest probability\n",
    "  third_largest_proba = sorted_proba[-3]\n",
    "\n",
    "  #define three variables each variable will hold the value of the next word first variable will be the most predicted value\n",
    "  next_word_1 = None\n",
    "  next_word_2 = None\n",
    "  next_word_3 = None\n",
    "\n",
    "  #this for loop will get the word as string\n",
    "  #value here is the id of the word and the key is the word itself\n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == largest_value_index[0]:\n",
    "          next_word_1 = key\n",
    "          continue\n",
    "      elif value == second_largest_value_index[0]:\n",
    "          next_word_2 = key\n",
    "          continue\n",
    "      elif value == third_largest_value_index[0]:\n",
    "          next_word_3 = key\n",
    "          continue\n",
    "\n",
    "  return f\"Correct word is [{correct_word}]\\n\\nthe predict next word will be one of these three words (the higher probability the higher chance to be the next word)\\n\\nthe predicted next word is [{next_word_1}] with the largest probability {round(largest_proba,3)}\\nthe next word could be [{next_word_2}] with the second largest probability {round(second_largest_proba,3)}\\nand it could be [{next_word_3}] with the third largest probability {round(third_largest_proba,3)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First trial: Simple recurrent language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:49:19.714700Z",
     "iopub.status.busy": "2022-04-05T19:49:19.713075Z",
     "iopub.status.idle": "2022-04-05T19:49:20.429172Z",
     "shell.execute_reply": "2022-04-05T19:49:20.428373Z",
     "shell.execute_reply.started": "2022-04-05T19:49:19.714652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "first_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#simple rnn\n",
    "#relu activation function to avoid vanishing gradient as much as we can\n",
    "RNN =  SimpleRNN(units = 1000)(embedded)\n",
    "\n",
    "#averaged = tf.reduce_mean(bi_lstm, axis=1)\n",
    "drop_out = Dropout(rate=0.2)(RNN)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "rnn_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "rnn_model.summary()\n",
    "tf.keras.utils.plot_model(rnn_model,to_file='first model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"simple_rnn_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n",
    "#To train the first model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the first model\n",
    "rnn_history = rnn_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting first model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(rnn_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "rnn_model = load_model('simple_rnn_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(rnn_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "first_end_time = time()\n",
    "total_time = first_end_time - first_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the first model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Trail: LSTM then gru layer to memorize sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:49:17.554143Z",
     "iopub.status.busy": "2022-04-05T19:49:17.553893Z",
     "iopub.status.idle": "2022-04-05T19:49:19.707899Z",
     "shell.execute_reply": "2022-04-05T19:49:19.706355Z",
     "shell.execute_reply.started": "2022-04-05T19:49:17.554108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "second_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#lstm layer\n",
    "lstm =  LSTM(units = 1000,return_sequences=True)(embedded)\n",
    "gru =  GRU(units = 736)(lstm)\n",
    "\n",
    "drop_out = Dropout(rate=0.2)(gru)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "second_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "second_model.summary()\n",
    "tf.keras.utils.plot_model(second_model,to_file='second model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "second_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"second_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n",
    "#To train the second model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the second model.\n",
    "second_history = second_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting second model preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(second_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "second_model = load_model('second_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(second_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "second_end_time = time()\n",
    "total_time = second_end_time - second_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the second model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third trial: GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:30:43.281708Z",
     "iopub.status.busy": "2022-04-05T19:30:43.281477Z",
     "iopub.status.idle": "2022-04-05T19:30:45.239198Z",
     "shell.execute_reply": "2022-04-05T19:30:45.238388Z",
     "shell.execute_reply.started": "2022-04-05T19:30:43.281682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "third_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#GRU layer\n",
    "gru =  GRU(units = 1000,return_sequences=True)(embedded)\n",
    "lstm =  LSTM(units = 736)(gru)\n",
    "\n",
    "drop_out = Dropout(rate=0.2)(lstm)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "third_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "third_model.summary()\n",
    "tf.keras.utils.plot_model(third_model,to_file='third model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:30:45.241446Z",
     "iopub.status.busy": "2022-04-05T19:30:45.240891Z",
     "iopub.status.idle": "2022-04-05T19:30:45.254663Z",
     "shell.execute_reply": "2022-04-05T19:30:45.253784Z",
     "shell.execute_reply.started": "2022-04-05T19:30:45.241405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "third_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:30:45.256731Z",
     "iopub.status.busy": "2022-04-05T19:30:45.256428Z",
     "iopub.status.idle": "2022-04-05T19:48:41.969223Z",
     "shell.execute_reply": "2022-04-05T19:48:41.968399Z",
     "shell.execute_reply.started": "2022-04-05T19:30:45.256694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"third_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n",
    "#To train the third model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the third model\n",
    "third_history = third_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting third model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:48:41.977627Z",
     "iopub.status.busy": "2022-04-05T19:48:41.972393Z",
     "iopub.status.idle": "2022-04-05T19:49:15.735070Z",
     "shell.execute_reply": "2022-04-05T19:49:15.734382Z",
     "shell.execute_reply.started": "2022-04-05T19:48:41.977588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(third_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the third model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:49:15.736521Z",
     "iopub.status.busy": "2022-04-05T19:49:15.736243Z",
     "iopub.status.idle": "2022-04-05T19:49:17.528298Z",
     "shell.execute_reply": "2022-04-05T19:49:17.527572Z",
     "shell.execute_reply.started": "2022-04-05T19:49:15.736470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "third_model = load_model('third_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(third_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the third model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:49:17.540219Z",
     "iopub.status.busy": "2022-04-05T19:49:17.539876Z",
     "iopub.status.idle": "2022-04-05T19:49:17.552670Z",
     "shell.execute_reply": "2022-04-05T19:49:17.551736Z",
     "shell.execute_reply.started": "2022-04-05T19:49:17.540185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "third_end_time = time()\n",
    "total_time = third_end_time - third_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the third model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth trial: Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:15:07.313142Z",
     "iopub.status.busy": "2022-04-05T19:15:07.312797Z",
     "iopub.status.idle": "2022-04-05T19:15:10.657211Z",
     "shell.execute_reply": "2022-04-05T19:15:10.656435Z",
     "shell.execute_reply.started": "2022-04-05T19:15:07.313107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fourth_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#Bi-directional LSTM layer\n",
    "bi_lstm =  Bidirectional(LSTM(units = 1000))(embedded)\n",
    "\n",
    "drop_out = Dropout(rate=0.2)(bi_lstm)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "fourth_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "fourth_model.summary()\n",
    "tf.keras.utils.plot_model(fourth_model,to_file='fourth model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:15:10.659422Z",
     "iopub.status.busy": "2022-04-05T19:15:10.658875Z",
     "iopub.status.idle": "2022-04-05T19:15:10.672236Z",
     "shell.execute_reply": "2022-04-05T19:15:10.671520Z",
     "shell.execute_reply.started": "2022-04-05T19:15:10.659379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fourth_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:15:10.673723Z",
     "iopub.status.busy": "2022-04-05T19:15:10.673378Z",
     "iopub.status.idle": "2022-04-05T19:30:20.752917Z",
     "shell.execute_reply": "2022-04-05T19:30:20.752195Z",
     "shell.execute_reply.started": "2022-04-05T19:15:10.673686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"fourth_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n",
    "#To train the fourth model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the fourth model\n",
    "fourth_history = fourth_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the fourth model preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:30:20.754526Z",
     "iopub.status.busy": "2022-04-05T19:30:20.754285Z",
     "iopub.status.idle": "2022-04-05T19:30:40.362005Z",
     "shell.execute_reply": "2022-04-05T19:30:40.361329Z",
     "shell.execute_reply.started": "2022-04-05T19:30:20.754481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(fourth_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the fourth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:30:40.363905Z",
     "iopub.status.busy": "2022-04-05T19:30:40.363432Z",
     "iopub.status.idle": "2022-04-05T19:30:43.278968Z",
     "shell.execute_reply": "2022-04-05T19:30:43.278198Z",
     "shell.execute_reply.started": "2022-04-05T19:30:40.363867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "fourth_model = load_model('fourth_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(fourth_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the fourth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:49:17.532079Z",
     "iopub.status.busy": "2022-04-05T19:49:17.531457Z",
     "iopub.status.idle": "2022-04-05T19:49:17.538436Z",
     "shell.execute_reply": "2022-04-05T19:49:17.537298Z",
     "shell.execute_reply.started": "2022-04-05T19:49:17.532036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fourth_end_time = time()\n",
    "total_time = fourth_end_time - fourth_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the fourth model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth trial: Bi-directional GRU to memorize the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T18:59:05.415177Z",
     "iopub.status.busy": "2022-04-05T18:59:05.414406Z",
     "iopub.status.idle": "2022-04-05T18:59:07.364116Z",
     "shell.execute_reply": "2022-04-05T18:59:07.363345Z",
     "shell.execute_reply.started": "2022-04-05T18:59:05.415132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fifth_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#Bi-directional GRU layer\n",
    "bi_gru =  Bidirectional(GRU(units = 1000))(embedded)\n",
    "\n",
    "drop_out = Dropout(rate=0.2)(bi_gru)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "fifth_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "fifth_model.summary()\n",
    "tf.keras.utils.plot_model(fifth_model,to_file='fifth model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:00:28.546169Z",
     "iopub.status.busy": "2022-04-05T19:00:28.545898Z",
     "iopub.status.idle": "2022-04-05T19:00:28.559256Z",
     "shell.execute_reply": "2022-04-05T19:00:28.558466Z",
     "shell.execute_reply.started": "2022-04-05T19:00:28.546138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fifth_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting training the fifth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:00:31.409135Z",
     "iopub.status.busy": "2022-04-05T19:00:31.408877Z",
     "iopub.status.idle": "2022-04-05T19:14:03.431093Z",
     "shell.execute_reply": "2022-04-05T19:14:03.430207Z",
     "shell.execute_reply.started": "2022-04-05T19:00:31.409107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"fifth_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=7)\n",
    "#To train the fifth model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the fifth model\n",
    "fifth_history = fifth_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the fifth model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T17:35:33.930756Z",
     "iopub.status.busy": "2022-04-05T17:35:33.930552Z",
     "iopub.status.idle": "2022-04-05T17:35:55.645162Z",
     "shell.execute_reply": "2022-04-05T17:35:55.644483Z",
     "shell.execute_reply.started": "2022-04-05T17:35:33.930731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(fifth_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the fifth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:15:04.452669Z",
     "iopub.status.busy": "2022-04-05T19:15:04.451805Z",
     "iopub.status.idle": "2022-04-05T19:15:07.301398Z",
     "shell.execute_reply": "2022-04-05T19:15:07.300696Z",
     "shell.execute_reply.started": "2022-04-05T19:15:04.452630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "fifth_model = load_model('fifth_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(fifth_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the fifth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:15:07.303688Z",
     "iopub.status.busy": "2022-04-05T19:15:07.303432Z",
     "iopub.status.idle": "2022-04-05T19:15:07.311301Z",
     "shell.execute_reply": "2022-04-05T19:15:07.310546Z",
     "shell.execute_reply.started": "2022-04-05T19:15:07.303654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fifth_end_time = time()\n",
    "total_time = fifth_end_time - fifth_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the fifth model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last trial: Bi-directional GRU connected with Bi-directional LSTM to memorize the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T19:14:03.432936Z",
     "iopub.status.busy": "2022-04-05T19:14:03.432697Z",
     "iopub.status.idle": "2022-04-05T19:15:04.450247Z",
     "shell.execute_reply": "2022-04-05T19:15:04.449416Z",
     "shell.execute_reply.started": "2022-04-05T19:14:03.432905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "last_start_time = time()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "in_text = keras.Input(batch_shape=(None, 50))\n",
    "\n",
    "#embedding layer uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "embedded = keras.layers.Embedding(vocab_size, 50)(in_text)\n",
    "#Bi-directional GRU layer\n",
    "bi_gru =  Bidirectional(GRU(units = 1000, return_sequences = True))(embedded)\n",
    "bi_lstm = Bidirectional(LSTM(units = 736))(bi_gru)\n",
    "\n",
    "drop_out = Dropout(rate=0.2)(bi_lstm)\n",
    "\n",
    "fused = Dense(128, activation='relu')(drop_out)\n",
    "output = Dense(vocab_size, activation='softmax')(fused)\n",
    "\n",
    "last_model = keras.Model(\n",
    "    inputs={\n",
    "        'previous_words': in_text,\n",
    "    },\n",
    "    outputs={\n",
    "        'next_word': output,\n",
    "    },\n",
    ")\n",
    "last_model.summary()\n",
    "tf.keras.utils.plot_model(last_model,to_file='last model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "last_model.compile(\n",
    "loss={\n",
    "    'next_word':\"categorical_crossentropy\"\n",
    "},\n",
    "metrics={\n",
    "    'next_word':[\"categorical_accuracy\"]\n",
    "},\n",
    "loss_weights={\n",
    "    'next_word':1,\n",
    "},\n",
    "optimizer=Nadam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"last_model.h5\", monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=7)\n",
    "#To train the last model on GPU\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "#Start training the last models\n",
    "last_history = last_model.fit(x={'previous_words': X}, y= {'next_word':y},validation_split=0.2, epochs=50, batch_size=256, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the last model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_performance(last_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start predicting the next word probability using the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#load the best model which gave us the best validation accuracy to predict the test data.\n",
    "last_model = load_model('last_model.h5')\n",
    "print('input data: '+test_lines[:-3]+' .....')\n",
    "print('\\n')\n",
    "print(predict_next_word_proba(last_model, tokenizer, test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to train and test the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "last_end_time = time()\n",
    "total_time = last_end_time - last_start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to train and test the last model was: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the total time taken to run the whole code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "end_time = time()\n",
    "total_time = end_time - start_time\n",
    "result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n",
    "print(f\"The total time taken to run this code was: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
